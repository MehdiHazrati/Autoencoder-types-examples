{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehdiHazrati/Autoencoder-types-examples/blob/main/data_gen_ptb_Jupyter(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3eoJ3yyCZKK",
        "outputId": "5a01b22a-7914-4a89-971c-9aba54b16d5e"
      },
      "id": "P3eoJ3yyCZKK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/gdrive/MyDrive/Colab Notebooks/ecg-id-database-1.0.0'"
      ],
      "metadata": {
        "id": "qkDm3NZjTTA3"
      },
      "id": "qkDm3NZjTTA3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wfdb"
      ],
      "metadata": {
        "id": "0a54LN71Cxf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eca4880d-dc4d-421c-9eb9-ae53a16b7261"
      },
      "id": "0a54LN71Cxf2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wfdb\n",
            "  Downloading wfdb-4.0.0-py3-none-any.whl (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 25.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.23.0)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.3.5)\n",
            "Requirement already satisfied: SoundFile<0.12.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (0.10.3.post1)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from wfdb) (3.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.7.3)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.10.1 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<4.0.0,>=3.2.2->wfdb) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=1.0.0->wfdb) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib<4.0.0,>=3.2.2->wfdb) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (2.10)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from SoundFile<0.12.0,>=0.10.0->wfdb) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->SoundFile<0.12.0,>=0.10.0->wfdb) (2.21)\n",
            "Installing collected packages: wfdb\n",
            "Successfully installed wfdb-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install progress"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2hOWZSEC-1X",
        "outputId": "59be4c2b-205f-4eeb-b64f-f98b9b576998"
      },
      "id": "H2hOWZSEC-1X",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting progress\n",
            "  Downloading progress-1.6.tar.gz (7.8 kB)\n",
            "Building wheels for collected packages: progress\n",
            "  Building wheel for progress (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progress: filename=progress-1.6-py3-none-any.whl size=9632 sha256=177b742b4a6b6d519c9132e4d809f7b30759c069c45c4ecbdf90784127fb431a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/d7/61/498d8e27dc11e9805b01eb3539e2ee344436fc226daeb5fe87\n",
            "Successfully built progress\n",
            "Installing collected packages: progress\n",
            "Successfully installed progress-1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a23cff8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a23cff8",
        "outputId": "7f13c069-df8f-4cb3-ff0e-1db1c6930082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[?25l\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/ecg-id-database-1.0.0/  -  Person\n",
            "90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing |################################| 100%\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptb.csv\n",
            "processing completed\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "import itertools\n",
        "import wfdb\n",
        "from wfdb import processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from progress.bar import Bar\n",
        "import heapq\n",
        "from scipy.stats.stats import pearsonr\n",
        "\n",
        "def dataGeneration(data_path, csv_path, record_path):\n",
        "\n",
        "    # initialize dataset\n",
        "    dataset = pd.DataFrame(columns=['label', 'record'])\n",
        "\n",
        "    if record_path == None:\n",
        "\n",
        "        # a loop for each patient\n",
        "        detail_path = data_path + '/'\n",
        "        record_files = [i.split('.')[0] for i in os.listdir(detail_path) if (not i.startswith('.') and i.endswith('.hea'))]\n",
        "\n",
        "        Bar.check_tty = False\n",
        "        bar = Bar('Processing', max=len(record_files), fill='#', suffix='%(percent)d%%')\n",
        "\n",
        "        # a loop for each record\n",
        "        for record_name in record_files:\n",
        "\n",
        "            # load record\n",
        "            signal, info = wfdb.rdsamp(detail_path + record_name)\n",
        "\n",
        "            fs = 200\n",
        "\n",
        "            signal = processing.resample_sig(signal[:,0], info['fs'], fs)[0]\n",
        "\n",
        "            # set some parameters\n",
        "            window_size_half = int(fs * 0.125 / 2)\n",
        "            max_bpm = 230\n",
        "\n",
        "            # detect QRS peaks\n",
        "            qrs_inds = processing.gqrs_detect(signal, fs=fs)\n",
        "            search_radius = int(fs*60/max_bpm)\n",
        "            corrected_qrs_inds = processing.correct_peaks(signal, peak_inds=qrs_inds, search_radius=search_radius, smooth_window_size=150)\n",
        "\n",
        "            average_qrs = 0\n",
        "            count = 0\n",
        "            for i in range(1, len(corrected_qrs_inds)-1):\n",
        "                start_ind = corrected_qrs_inds[i] - window_size_half\n",
        "                end_ind = corrected_qrs_inds[i] + window_size_half + 1\n",
        "                if start_ind<corrected_qrs_inds[i-1] or end_ind>corrected_qrs_inds[i+1]:\n",
        "                    continue\n",
        "                average_qrs = average_qrs + signal[start_ind: end_ind]\n",
        "                count = count + 1\n",
        "\n",
        "            # remove outliers\n",
        "            if count < 8:\n",
        "                print('\\noutlier detected, discard ' + record_name)\n",
        "                continue\n",
        "\n",
        "            average_qrs = average_qrs / count\n",
        "\n",
        "            corrcoefs = []\n",
        "            for i in range(1, len(corrected_qrs_inds)-1):\n",
        "                start_ind = corrected_qrs_inds[i] - window_size_half\n",
        "                end_ind = corrected_qrs_inds[i] + window_size_half + 1\n",
        "                if start_ind<corrected_qrs_inds[i-1] or end_ind>corrected_qrs_inds[i+1]:\n",
        "                    corrcoefs.append(-100)\n",
        "                    continue\n",
        "                corrcoef = pearsonr(signal[start_ind: end_ind], average_qrs)[0]\n",
        "                corrcoefs.append(corrcoef)\n",
        "\n",
        "            max_corr = list(map(corrcoefs.index, heapq.nlargest(8, corrcoefs)))\n",
        "\n",
        "            index_corr = random.sample(list(itertools.permutations(max_corr, 8)), 100)\n",
        "\n",
        "            for index in index_corr:\n",
        "                # a temp dataframe to store one record\n",
        "                record_temp = pd.DataFrame()\n",
        "\n",
        "                signal_temp = []\n",
        "\n",
        "                for i in index:\n",
        "                    start_ind = corrected_qrs_inds[i + 1] - window_size_half\n",
        "                    end_ind = corrected_qrs_inds[i + 1] + window_size_half + 1\n",
        "                    sig = processing.normalize_bound(signal[start_ind: end_ind], -1, 1)\n",
        "                    signal_temp = np.concatenate((signal_temp, sig))\n",
        "\n",
        "                record_temp = record_temp.append(pd.DataFrame(signal_temp.reshape(-1,signal_temp.shape[0])), ignore_index=True, sort=False)\n",
        "                record_temp['label'] = record_name\n",
        "                record_temp['record'] = record_name\n",
        "\n",
        "                # add it to final dataset\n",
        "                dataset = dataset.append(record_temp, ignore_index=True, sort=False)\n",
        "                \n",
        "            bar.next()    \n",
        "        bar.finish()    \n",
        "    else:\n",
        "        patient_folders = [i for i in os.listdir(data_path) if (not i.startswith('.') and i.startswith(record_path))]\n",
        "\n",
        "        print(data_path, ' - ',record_path)\n",
        "\n",
        "        Bar.check_tty = False\n",
        "        bar = Bar('Processing', max=len(patient_folders), fill='#', suffix='%(percent)d%%')\n",
        "        # a loop for each patient\n",
        "        print(len(patient_folders))\n",
        "        for patient_name in patient_folders:\n",
        "            detail_path = data_path + patient_name + '/'\n",
        "            record_files = [i.split('.')[0] for i in os.listdir(detail_path) if i.endswith('.hea')]\n",
        "\n",
        "            # a loop for each record\n",
        "            for record_name in record_files:\n",
        "\n",
        "                # load record\n",
        "                signal, info = wfdb.rdsamp(detail_path + record_name)\n",
        "\n",
        "                fs = 200\n",
        "\n",
        "                signal = processing.resample_sig(signal[:,0], info['fs'], fs)[0]\n",
        "\n",
        "                # set some parameters\n",
        "                window_size_half = int(fs * 0.125 / 2)\n",
        "                max_bpm = 230\n",
        "\n",
        "                # detect QRS peaks\n",
        "                qrs_inds = processing.gqrs_detect(signal, fs=fs)\n",
        "                search_radius = int(fs*60/max_bpm)\n",
        "                corrected_qrs_inds = processing.correct_peaks(signal, peak_inds=qrs_inds, search_radius=search_radius, smooth_window_size=150)\n",
        "\n",
        "                average_qrs = 0\n",
        "                count = 0\n",
        "                for i in range(1, len(corrected_qrs_inds)-1):\n",
        "                    start_ind = corrected_qrs_inds[i] - window_size_half\n",
        "                    end_ind = corrected_qrs_inds[i] + window_size_half + 1\n",
        "                    if start_ind<corrected_qrs_inds[i-1] or end_ind>corrected_qrs_inds[i+1]:\n",
        "                        continue\n",
        "                    average_qrs = average_qrs + signal[start_ind: end_ind]\n",
        "                    count = count + 1\n",
        "\n",
        "                # remove outliers\n",
        "                if count < 8:\n",
        "                    print('\\noutlier detected, discard ' + record_name + ' of ' + patient_name)\n",
        "                    continue\n",
        "\n",
        "                average_qrs = average_qrs / count\n",
        "\n",
        "                corrcoefs = []\n",
        "                for i in range(1, len(corrected_qrs_inds)-1):\n",
        "                    start_ind = corrected_qrs_inds[i] - window_size_half\n",
        "                    end_ind = corrected_qrs_inds[i] + window_size_half + 1\n",
        "                    if start_ind<corrected_qrs_inds[i-1] or end_ind>corrected_qrs_inds[i+1]:\n",
        "                        corrcoefs.append(-100)\n",
        "                        continue\n",
        "                    corrcoef = pearsonr(signal[start_ind: end_ind], average_qrs)[0]\n",
        "                    corrcoefs.append(corrcoef)\n",
        "\n",
        "                max_corr = list(map(corrcoefs.index, heapq.nlargest(8, corrcoefs)))\n",
        "\n",
        "                index_corr = random.sample(list(itertools.permutations(max_corr, 8)), 100)\n",
        "\n",
        "                for index in index_corr:\n",
        "                    # a temp dataframe to store one record\n",
        "                    record_temp = pd.DataFrame()\n",
        "\n",
        "                    signal_temp = []\n",
        "\n",
        "                    for i in index:\n",
        "                        start_ind = corrected_qrs_inds[i + 1] - window_size_half\n",
        "                        end_ind = corrected_qrs_inds[i + 1] + window_size_half + 1\n",
        "                        sig = processing.normalize_bound(signal[start_ind: end_ind], -1, 1)\n",
        "                        signal_temp = np.concatenate((signal_temp, sig))\n",
        "\n",
        "                    record_temp = record_temp.append(pd.DataFrame(signal_temp.reshape(-1,signal_temp.shape[0])), ignore_index=True, sort=False)\n",
        "                    record_temp['label'] = patient_name\n",
        "                    record_temp['record'] = record_name\n",
        "\n",
        "                    # add it to final dataset\n",
        "                    dataset = dataset.append(record_temp, ignore_index=True, sort=False)\n",
        "                \n",
        "            bar.next()    \n",
        "        bar.finish()\n",
        "\n",
        "    # save for further use\n",
        "    print(csv_path)\n",
        "    dataset.to_csv(f'{PATH}/{csv_path}', index=False)\n",
        "\n",
        "    print('processing completed')\n",
        "\n",
        "\n",
        "dataset_name = PATH\n",
        "record_path = 'Person'#'patient'\n",
        "# root path\n",
        "data_path = dataset_name + '/'\n",
        "\n",
        "csv_path = 'ptb.csv'\n",
        "\n",
        "dataGeneration(data_path, csv_path, record_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s40mR7H0YbQf"
      },
      "id": "s40mR7H0YbQf",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}